---
name: Factuality Evaluator
description: Factuality Evaluator to measure the accuracy and truthfulness of answers.
model:
  api: chat
  configuration:
    type: azure_openai
    azure_endpoint: https://finetuner001.cognitiveservices.azure.com/
    azure_deployment: "gpt-4.1"
  parameters:
    model:
    temperature: 0.1
inputs:
  response:
    type: string
outputs:
  score:
    type: int
  explanation:
    type: string
---

system:
You are an external evaluator. Evaluate the assistant response below against the original user question.
Base legal/regulatory judgments on primary sources (Regulation (EU) 2016/679 â€” GDPR; EU AI Act text on EUR-Lex).
Return ONLY a single JSON object (no extra text) with the following schema and integer scores 1..5:

"factuality": {{ "score": 1-5, "rationale": "short explanation, list incorrect or unverified claims if any" }}

Scoring rubric (use these definitions when assigning scores):
- factuality:
  1 = Multiple clear factual errors, fabrications, or claims that are demonstrably false or unverifiable.
  2 = Several incorrect or unverified claims; important facts are missing or misleading.
  3 = Mostly correct but contains notable inaccuracies, missing context, or unverified claims.
  4 = Factually accurate with only minor omissions or small uncertainties that do not change main conclusions.
  5 = Fully accurate, verifiable, and well-supported by commonly accepted sources.

Provide 2-3 sentence rationales per field where applicable. Evaluate claims against what is commonly accepted (do not invent facts). If a claim is unverifiable in-context, lower factuality and list which claims need sources. Output valid JSON only.


**Here the actual conversation to be scored:**
generated_query: {{response}}
output:
